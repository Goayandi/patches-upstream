From ca1e95911bfa6c13fe146c7253e8eed5be3f4c9c Mon Sep 17 00:00:00 2001
From: Morten Rasmussen <morten.rasmussen@arm.com>
Date: Wed, 4 Feb 2015 18:31:11 +0800
Subject: [PATCH 34/48] sched: Bias new task wakeups towards higher capacity
 cpus

Make wake-ups of new tasks (find_idlest_group) aware of any differences
in cpu compute capacity so new tasks don't get handed off to a cpus with
lower capacity.

cc: Ingo Molnar <mingo@redhat.com>
cc: Peter Zijlstra <peterz@infradead.org>

Signed-off-by: Morten Rasmussen <morten.rasmussen@arm.com>
---
 kernel/sched/fair.c | 15 +++++++++++++--
 1 file changed, 13 insertions(+), 2 deletions(-)

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index c6e6a5a..d4c1d41 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -4946,6 +4946,7 @@ find_idlest_group(struct sched_domain *sd, struct task_struct *p,
 {
 	struct sched_group *idlest = NULL, *group = sd->groups;
 	unsigned long min_load = ULONG_MAX, this_load = 0;
+	unsigned long this_cpu_cap = 0, idlest_cpu_cap = 0;
 	int load_idx = sd->forkexec_idx;
 	int imbalance = 100 + (sd->imbalance_pct-100)/2;
 
@@ -4953,7 +4954,7 @@ find_idlest_group(struct sched_domain *sd, struct task_struct *p,
 		load_idx = sd->wake_idx;
 
 	do {
-		unsigned long load, avg_load;
+		unsigned long load, avg_load, cpu_capacity;
 		int local_group;
 		int i;
 
@@ -4967,6 +4968,7 @@ find_idlest_group(struct sched_domain *sd, struct task_struct *p,
 
 		/* Tally up the load of all CPUs in the group */
 		avg_load = 0;
+		cpu_capacity = 0;
 
 		for_each_cpu(i, sched_group_cpus(group)) {
 			/* Bias balancing toward cpus of our domain */
@@ -4976,6 +4978,9 @@ find_idlest_group(struct sched_domain *sd, struct task_struct *p,
 				load = target_load(i, load_idx);
 
 			avg_load += load;
+
+			if (cpu_capacity < capacity_of(i))
+				cpu_capacity = capacity_of(i);
 		}
 
 		/* Adjust by relative CPU capacity of the group */
@@ -4983,14 +4988,20 @@ find_idlest_group(struct sched_domain *sd, struct task_struct *p,
 
 		if (local_group) {
 			this_load = avg_load;
+			this_cpu_cap = cpu_capacity;
 		} else if (avg_load < min_load) {
 			min_load = avg_load;
 			idlest = group;
+			idlest_cpu_cap = cpu_capacity;
 		}
 	} while (group = group->next, group != sd->groups);
 
-	if (!idlest || 100*this_load < imbalance*min_load)
+	if (!idlest)
+		return NULL;
+
+	if (100*this_load < imbalance*min_load && this_cpu_cap >= idlest_cpu_cap)
 		return NULL;
+
 	return idlest;
 }
 
-- 
1.9.1

